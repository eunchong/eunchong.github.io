---
title:  "[회고] 버즈니에서 검색 백엔드 엔지니어로 사는 이야기"
search: true
categories: 
  - 생각정리
tags:
  - 2020_회고
  - 회고
  - 생각정리
classes: wide
---


**2019.03~** 버즈니에서 1년 9개월이라는 시간이 지났다. 찰리라는 닉네임으로 백엔드 엔지니어 역할을 맡고있다.  
<!-- 과거 소프트웨어 마에스트로에서 인연을 맺은 저스틴의 추천으로 오랜 열정을 쏟아왔던 해킹/보안 분야를 떠나 버즈니 검색팀에 입사했다.   -->

---
<!-- 몇일 전, 경영진과의 티타임에서 저스틴과 피터가 나는 커리어에 대해 어떤 생각을 갖고있는지 생각을 물었다. -->
<!-- 나는 앞으로 어떤 길을 가야할까? 내가 바라본 버즈니는 어떤 회사였을까? 나와 버즈니는 성장했나? -->

## 검색 백엔드 엔지니어, 찰리
<!-- ### 성장에 한계가 없는 버즈니 -->

검색팀에 함께한 1년 6개월동안, APM, 프로파일러를 활용하여 장애의 주요 원인들을 찾고, failover 시스템, 서킷브레이커 구축을 통해 단기적 개선을 하고, ES2→ES7 마이그레이션으로 근본적인 개선을 하는 길고도 짧은 한 사이클을 경험했다. 처음엔 막연하게 서비스를 개발하고 싶었다. 자유롭고 주도적인 시도를 할 수 있는 버즈니가 좋았다.

### 장애 원인 파악
검색 slow의 주요 원인을 찾는 경험을 했다. 파이썬 프로파일링 도구, APM 도구들을 활용했다. elastic apm을 falcon에서 쉽게 사용할 수 있도록 간단한 middleware 라이브러리 형태로 만들어서 적용했다.  

`falcon elastic apm 라이브러리 구현` [(github)](https://github.com/eunchong/falcon-elastic-apm)

> APM을 활용하면 어떤 요청에서 latency가 높은지 디테일한 로그들을 볼 수 있어서 좋았다.

### failover 시스템 구축
[네이버 메인 페이지의 트래픽 처리](https://d2.naver.com/helloworld/6070967) 라는 글을 재미있게 읽었다. 지진 등 자연재해로 인해 네이버 메인의 트래픽이 급증할때 failover 시스템을 구축해서 대응했다는 사례였다. 버즈니 검색에서 비슷한 사례를 구축할 수 있을까? 작은 도전적인 목표가 생겼다. 회사 자체가 무언가 도전하기 워낙 자유로운 분위기다. 버즈니 검색팀에 failover 시스템을 구축했다.

`파이썬 API 앞단 nginx 레이어에 서킷브레이커 failover 시스템 구축`

> nginx 문서를 살펴보니 proxy_next_upstream 이라는 훌륭한 기능이 있었다. emergency cache system에서 캐싱 결과가 없는 트래픽은 의도적으로 400 exception을 발생시키도록 했다. 해당 트래픽들은 nginx proxy_next_upstream을 통해 실 서비스 API로 전달되었다. 생각보다 심플한 코드로 금방 프로토타입 형태가 만들어졌다. 테스트를 진행해보니 긴급 상황에 약 70% 이상의 트래픽을 캐싱 시스템에서 처리할 수 있다는 결과를 확인했다. 해당 부분들을 팀에 정리해서 건의했더니, 매우 긍정적인 피드백이 있었다. 처음 구현 당시 부족한 부분도 많았었는데, 시스템을 더욱 안정적으로 발전시킬 수 있도록 팀에서 격려와 응원을 해주었다. 당시 함께 검색 백엔드 영역에서 고군분투했던 내 옆자리의 티거는 **서킷브레이커**에 **ES 산소호흡기**라는 별명을 붙여주었다. 이 산소호흡기 덕분에, 트래픽이 급증하는 저녁 시간도 긴급하게 맥북을 켜거나 슬랙으로 장애를 알리거나 ES 클러스터들을 재부팅하는 등의 노력을 할 필요가 없어졌다. 그리고 홈쇼핑모아 유저들이 조금 더 쾌적한 서비스를 이용할 수 있다는 부분이 뿌듯했다. 덕분에 퇴근 후 시간에 개인적인 시간과 여유를 가질 수 있었다. **여자친구랑 사이가 좋아졌다. 행복지수 상승!**

### 검색 로직 개선
ES CPU가 느려지는 주요한 원인들을 찾고 싶었다. 레거시 코드중 동의어 관련 로직과 랭킹스크립트 관련 로직에서 비효율적인 부분들을 찾았고 개선했다.

`동의어 관련 로직 개선`
`랭킹스크립트 적용 대상 최적화`

> 일 평균 6회 이상 동작했던 **서킷브레이커는 일 평균 1회 동작**하는 상태로 일부 안정되었다.

### ES2→ES7 마이그레이션
es major 버전을 2에서 7로 올리는 과정을 진행했다. 과정에서 데이터 누락, 의도치 않은 버그, 사이드이팩트가 발생하지 않을지 걱정이 있었다. 메트릭들을 촘촘히 구성해서 모니터링했다. 샘플링 데이터들을 만들어서 테스트를 돌려보고, 1~2주간 a/b test를 진행하는 과정을 통해 크고 작은 리스크들을 제거했다. 색인 마이그레이션 과정에서 속도를 내기위해 함께 TF를 만들어서 3일간 행아웃으로 데이터팀 데미안과 함께 페어프로그래밍을 진행했다.

<center><img src="/assets/img/2021_buzzni/failover_system.png" style="border: 1px solid" width="510px" > <img src="/assets/img/2021_buzzni/es_migration_roadmap.png" style="border: 1px solid" width="300px" ></center>  
<br>
`deprecated query 개선` `인덱스 도큐먼트 구조 개선` `groovy 기반 랭킹스크립트 painless 변경`  
`query_dsl 동적생성 파이썬 코드 리팩토링(>2000 Line)` `세그멘터(외부 api 형태) 교체` `aws cloudwatch 메트릭 추가(>20개)`  
`누락 데이터 모니터링 대시보드` `pandas 데이터 분석 (10000개 query 샘플 대상)` `A/B 테스트` `실 서비스 전체 배포`

> 실 서비스 배포 이후 p99 latency는 **기존 4초에서 0.7초로 개선**되었고, **서킷브레이커는 이후 0회 동작했다.**

<!-- > 데이터팀의 데미안과 페어프로그래밍을 했다. 우여곡절 끝에 2분기 가량의 짧고도 긴 시간을 보내며 **ES2→ES7 마이그레이션**을 진행했다. 마이그레이션 과정에서 10개가 넘는 쇼핑사들의 상품들에 대한 누락은 없는지, 백프로세스들에서 지속적으로 업데이트 하는 데이터에 지연이나 누락은 없는지 확인이 필요해서 aws cloudwatch에 20개 이상의 메트릭을 촘촘하게 추가해서 모니터링 했다. 대시보드를 통해 시계열 추세를 확인하면, 예상치 못한 색인 프로세스, 백프로세스의 마이그레이션 이슈로 인해 누락되는 데이터들을 쉽고 빠르게 확인할 수 있어서 좋았다. 추가로 검색결과와 랭킹들을 비교 분석하며 크고 작은 로직 버그와 오류들을 수정해나가는 과정이 필요했다. 임의로 샘플링한 1000~10000개의 top query, long tail query 결과들을 jupyter notebook에 올려놓고 pandas를 통해 분석하는 방법을 시도해보았다. 이 과정 덕분에 실 서비스를 태워보기 전에 예상치 못한 20~30개 이상의 숨겨진 크고작은 이슈들을 찾고 개선할 수 있었다. 마지막으로 A/B 테스트를 통해 클릭률과 클러스터 부하에 나쁜 영향은 없는지 의도치 않은 이슈는 없는지 검증을 진행했고, 문제가 없다는 판단에 20년 11월 05일 최종적으로 ES7 전체배포를 진행했다. 진행 과정에서 es 마이그레이션만으로 성능이 개선 될지 반신반의했다. 사실 최신 es(>=7.0)의 효율을 극대화하기 위해서는, 무분별한 랭킹스크립트의 사용을 줄이고 rank_feature 등의 신규 기능들을 활용하여, es 내부에서 검색 스코어링의 후보군을 획기적으로 줄이는 block max wand 최적화 기능을 최대한 활용하는게 좋아보였다. 하지만 단순한 es2->es7 마이그레이션 과정으로도 상당한 개선이 있었다. 아마 다양한 원인을 추측할 수 있겠지만, es2에서 사용되던 groovy가 es7에서 painless로 수정된 부분이 성능 개선의 큰 이유를 차지하지 않았을지 조심스레 추측해본다. 아무튼, 덕분에 p99 latency는 기존 4초에서 0.7초로 크게 개선되었고, 최초 일 평균 6회, 개선 후 일 평균 1회 동작했던 서킷브레이커는 es7 전체배포 이후 한번도 동작하지 않고 조용한 상태가 되었다. 진행 과정에서 클릭률 하락, 데이터 누락, 의도치 않은 버그, 사이드이팩트에 대한 걱정과 고민도 많았지만, 메트릭을 분석하고 샘플링 데이터를 통해 테스트를 돌려보고 1~2주간 a/b test를 진행하며 크고 작은 리스크들을 제거할 수 있어서 좋았다. 무엇보다 도전적인 문제를 주도적으로 풀어볼 수 있는 기회와 자유로운 환경이 좋았으며 감사했다. 그리고 색인 마이그레이션 과정에서 함께 TF를 만들어 행아웃으로 페어프로그래밍을 진행했던 데이터팀 **데미안 👍**과 함께 한 열정의 시간들이 너무 즐거웠다. 검색팀 이안, 티거와 호흡하고, 고민하며 문제들을 해결하는 과정들도 즐거웠고 중간중간 테스트 리소스, 개발 환경이 필요한 상황, 혼자 힘으로 해결하기 어려운 이슈들이 있었는데 그때마다 장애물들을 빠르게 해결해주고 가이드도 제시해주는 든든한 CTO 제이콥도 항상 최고다. -->

### 집중이 안됐던 순간들
집중이 안될때는 11층에 내려가서 자유롭게 잠을 자거나 탁구를 치거나 다트를 던지며 휴식을 취할수 있었다. 버즈니는 자유롭게, 주도적으로 생각한 아이디어들을 시도해볼 수 있는 분위기를 갖고있다. 최근에는 직원수가 많아지면서 많은 인원이 활용 가능한 3층의 버즈니 라운지 형태로 개편이 되었다. 이로인해 탁구대는 없어졌지만, 피곤할때 잠을 자거나 집중이 안될때 내려가서 다트를 던질수 있다.  

<center><img src="/assets/img/2021_buzzni/dart01.jpeg" style="border: 1px solid" width="300" ><img src="/assets/img/2021_buzzni/dart03.jpeg" style="border: 1px solid" width="300" ></center>

> 이안과 함께 하던 어느날, **BULL을 연속으로 3번** 맞추는 목표에 성공! (다트 꿀잼)

### 그리고 커머스팀
지금은 1년 6개월간의 검색팀을 지나 지금은 커머스팀에서 백엔드 역할을 하고있다. 회원 시스템 개선, 신규 기능 추가 업무를 하고있다. 새로운 커머스의 한 사이클에서는 어떤 경험들이 함께할지, 긴장도 되고 기대도 된다. 버즈니 커머스 화이팅!  

`레거시 유저 시스템 개선`

<!-- **TechShare**  
```삽질을 공유하는 시간들이 즐겁다.```

**반기, 연말 회고쓰기**  
```버즈니는 반기, 연말에 회고를 쓰고 피어 평가를 하는 문화가 있다. 매년 한일들을 누군가가 읽는 하나의 글로 정리하는 과정은 솔직히 조금 귀찮은 일인데, 회고라는 과정 덕분에 매년 생각을 정리하게 돼서 나 스스로에게도 여러모로 유익하다는 생각이 있다. 솔직히 지금 이 블로그를 쓰는 순간도 엄청나게 귀찮아서 이 행위를 포기하고 싶다는 생각을 1억만번 했다. 귀차니즘 만렙이 내 본성인지라, 지금도 그냥 때려치고 코딩하고싶다는 생각이 2억만번 들고 있다. 그런데 버즈니에서 회고를 쓰는 습관이 내가 블로그에 회고라는걸 끝까지 쓰게 만든것 같다. 이 글을 마무리하는걸 성공할수있을까? 크리스마스떄 먹다 남은 스파클링 와인을 먹고 힘을내서 마무리 하는중... 화이팅!``` -->

### 버즈니 채용 추천 문의
> 버즈니에서 함께 성장하실 인재분들은 성장의 즐거움이 있는 버즈니로 오세요~  
> 추천인 이메일: charlie@buzzni.com


